# Weekly Aggregator

Этот проект представляет собой проект на Airflow c еженедельной Spark задачей, который считает недельный агрегат на заданную дату и записывает результаты в соответствующий CSV файл.
К сожалению на данный момент запуск dag файла выдает в логах ошибку, не получается настроить разрешения для папок, но я над этим работаю

## Установка и запуск

1. Установите docker desktop на ваш компьютер, если он еще не установлен.
2. Скачайте и распакуйте проект в папку на вашем компьютере.
3. Откройте терминал или командную строку и перейдите в папку проекта.
4. Для генерации входных данных используется [скрипт](https://gist.github.com/afazulzyanov/c4cff39c216be63c10be27d39b0577d8).
5. Входные scv файлы должны находиться в корневой директории input.
6. В терминале выполните ````docker-compose up -d --build````. Базы данных попросят инициализироваться, для это выполните шаг 7.
7. Выполните ````docker-compose up -d````.
8. Веб-сервер airflow будет доступен по [http://localhost:8080/](http://localhost:8080/).
9. Логин и пароль: admin admin.
10. В admin/connections создайте новое соединение с параметрами:
- Connection Id:  spark-conn
- Connection Type:  Spark
- Host: spark://spark-master
- Port: 7077
11. Запустите задачу dag_weekly_aggr
  
## Функционал
При запуске скрипта происходит проверка на существование промежуточных агрегированных файлов за семь предыдущих дней. Если файлы пропущены или не обнаружены производится их генерация, после чего скрипт обращается к уже агрегированным файлам и создаёт файл .csv с недельным агрегатом на требуемую дату.
- На данном этапе написан task на scala и в локальной машине он работает при условии, что в файле CsvAggregator.scala поменять в 11 строчке на ````.master(local[*])```` и заново скомпилировать jar при помощи ````sbt compile````, а затем выполнить ````sbt run````

## Пока запуск не происходит
В ветке develop-python представлен скрипт на python выполняющий ту же задачу, но без планировщика
